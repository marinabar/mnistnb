{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Part 3 - Training Neural Networks (Exercises).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUtWRQoJWYIK",
        "colab_type": "text"
      },
      "source": [
        "# Обучение нейросети\n",
        "\n",
        "\n",
        "Сеть, которую мы создали в предыдущей части, не очень умна, она ничего не знает о наших рукописных цифрах. Нейронные сети с нелинейными активациями работают как универсальные аппроксиматоры функций. Есть некоторая функция, которая отображает ваш вход в какой-то выход. Например, изображения рукописных цифр в вероятности классов. Мощь нейронных сетей заключается в том, что мы можем обучить их приближать эту функцию, и в целом, любую функцию, если есть достаточно данных и времени.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "Сначала сеть случайна, она не знает функции, сопоставляющей входы и выходы. Мы обучаем сеть, показывая ей примеры реальных данных, а затем корректируем параметры сети так, чтобы она приближалась к этой функции.\n",
        "\n",
        "Чтобы найти эти параметры, нам нужно знать, насколько плохо сеть прогнозирует реальные результаты. Для этого мы рассчитываем **функцию потерь** (также называемую **loss**), меру нашей ошибки прогнозирования. Например, среднеквадратичную ошибку часто используются в задачах регрессии и двоичной классификации.\n",
        "\n",
        "\n",
        "$$\n",
        "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
        "$$\n",
        "\n",
        "\n",
        "где $n$ это число обучающих примеров, $y_i$ это истинные метки, а $\\hat{y}_i$ это предсказанные метки.\n",
        "\n",
        "\n",
        "Минимизируя эту функцию ошибки по отношению к параметрам сети, мы можем найти конфигурации, в которых ошибка минимальна, и сеть способна предсказать правильные метки с высокой точностью. Мы находим этот минимум, используя процесс под названием **градиентный спуск**. Градиент является наклоном функции потерь и указывает в направлении наискорейшего роста. Чтобы достичь минимума как можно быстрее, мы хотим следовать антиградиенту (вниз). Вы можете думать об этом как о спуске с горы, следуя по самому крутому склону к основанию.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axGpWq_FWYIR",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Для однослойных сетей, градиентный спуск прост в реализации. Однако это более сложно для более глубоких, многослойных нейронных сетей, подобных той, которую мы создали. Достаточно сложно, что понадобилось около 30 лет, чтобы исследователи выяснили, как обучать многослойные сети.\n",
        "\n",
        "Обучение многослойных сетей осуществляется с помощью **метода обратного распространения ошибки**, который на самом деле является просто применением правила производной сложной функции из матанализа. Это проще всего понять, если преобразовать двухслойную сеть в графическое представление.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "При прямом прохождении через сеть наши данные и операции идут здесь снизу вверх. Мы передаем вход $ x $ через линейное преобразование $ L_1 $ с весами $ W_1 $ и смещениями $ b_1 $. Выходные данные затем проходят через сигмоид $ S $ и другое линейное преобразование $ L_2 $. Наконец, мы рассчитываем ошибку $ \\ell $. Мы используем ошибку как меру того, насколько плохи прогнозы сети. Тогда цель состоит в том, чтобы отрегулировать веса и смещения, чтобы минимизировать потери.\n",
        "\n",
        "Чтобы тренировать веса с градиентным спуском, мы распространяем градиент потерь обратно по сети. Каждая операция имеет некоторый градиент между входами и выходами. Когда мы отправляем градиенты назад, мы умножаем входящий градиент на градиент операции. Математически это действительно просто вычисление градиента ошибки по весам с использованием правила производной сложной функции.\n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
        "$$\n",
        "\n",
        "\n",
        "Мы обновляем наши веса с шагом $\\alpha$. \n",
        "\n",
        "$$\n",
        "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtdKKw4zWYIV",
        "colab_type": "text"
      },
      "source": [
        "## Функции ошибок в PyTorch\n",
        "\n",
        "Давайте начнем с того, что посчитаем ошибку с помощью PyTorch. Через модуль `nn` PyTorch позволяет считать подобные функции. Например, ошибка кросс-энтропии (` nn.CrossEntropyLoss`). Обычно за функцию потерь отвечает параметр `criterion`. Как отмечалось в предыдущей части, с такой задачей классификации, как MNIST, мы используем функцию softmax для прогнозирования вероятностей классов. С выходом softmax мы используем кросс-энтропию в качестве функции потерь. \n",
        "\n",
        "[Документация `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
        "\n",
        "> Эта функция комбинирует `nn.LogSoftmax()` и `nn.NLLLoss()` в одном классе.\n",
        ">\n",
        "> Под входом ожидаются числовые оценки классов (scores)\n",
        "\n",
        "Это означает, что нам нужно передать необработанный вывод нашей сети в лосс, а не вывод функции softmax. Этот необработанный вывод обычно называется *logits* (логиты) или *scores*. Мы используем логиты, потому что softmax дает вам вероятности, которые часто будут очень близки к нулю или единице, но числа с плавающей точкой не могут точно представлять значения, близкие к нулю или единице (https://docs.python.org/3/tutorial/floatingpoint.html ). Обычно лучше избегать вычислений с вероятностями, поэтому мы используем логарифмические вероятности.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOrNKUOdWYIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JecLm9bWYIq",
        "colab_type": "code",
        "outputId": "94261c0d-f97f-40be-f412-643b77adbd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10))\n",
        "\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Get our data\n",
        "images, labels = next(iter(trainloader))\n",
        "# Flatten images\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "# Forward pass, get our logits\n",
        "logits = model(images)\n",
        "# Calculate the loss with the logits and the labels\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3016, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cZ2Oq9pWYI2",
        "colab_type": "text"
      },
      "source": [
        "Возможно, удобнее считать лосс с `nn.LogSoftmax` или `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Для получения вероятностей, нужно пропустить полученные значения через экспоненту `torch.exp(output)`. С таким выводом нужно использовать negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)).\n",
        "\n",
        ">**Упражнение:** Создайте модель, которая возвращает log-softmax в качестве выходных данных и рассчитайте потери, используя negative log likelihood loss. Обратите внимание, что для `nn.LogSoftmax` и` F.log_softmax` вам необходимо соответствующим образом установить аргумент ключевого слова `dim`. `dim = 0` вычисляет softmax по строкам, и каждый столбец суммируется в 1, в то время как` dim = 1` вычисляет по столбцам, и каждая строка суммируется в 1. Подумайте, каким должен быть вывод, и выберите `dim` соответствующим образом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YrdeaXrWYI7",
        "colab_type": "code",
        "outputId": "f00e7dd0-ccf4-4a14-991f-5971524524c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: Build a feed-forward network\n",
        "model= nn.Sequential(nn.Linear(784, 128),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(128, 64),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(64, 10),\n",
        "                     nn.LogSoftmax(dim=1))\n",
        "\n",
        "# TODO: Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "### Run this to check your work\n",
        "# Get our data\n",
        "images, labels = next(iter(trainloader))\n",
        "# Flatten images\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "# Forward pass, get our logits\n",
        "logits = model(images)\n",
        "# Calculate the loss with the logits and the labels\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3040, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aussagbUWYJH",
        "colab_type": "text"
      },
      "source": [
        "## Autograd\n",
        "\n",
        "Теперь, когда мы знаем, как рассчитать потери, как мы можем использовать их для обратного распространения? Torch предоставляет модуль autograd для автоматического расчета градиентов тензоров. Мы можем использовать его для расчета градиентов всех наших параметров относительно потерь. Автоград работает, отслеживая операции, выполняемые над тензорами, а затем проходит назад по этим операциям, вычисляя градиенты по пути. Чтобы PyTorch отслеживал операции над тензором и вычислял градиенты, вам нужно установить `require_grad = True` для тензора. Вы можете сделать это при создании с помощью ключевого слова `require_grad` или в любое время с помощью `x.requires_grad_(True)`.\n",
        "\n",
        "Вы можете отключить градиенты для блока кода с помощью содержимого `torch.no_grad ()`:\n",
        "\n",
        "```python\n",
        "x = torch.zeros(1, requires_grad=True)\n",
        ">>> with torch.no_grad():\n",
        "...     y = x * 2\n",
        ">>> y.requires_grad\n",
        "False\n",
        "```\n",
        "\n",
        "Кроме того, вы можете включить или выключить градиенты используя `torch.set_grad_enabled (True | False)`.\n",
        "\n",
        "Градиенты вычисляются относительно некоторой переменной `z` с помощью` z.backward() `. Это делает обратный проход через операции, результат которых хранится в `z`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdTxlP5WYJM",
        "colab_type": "code",
        "outputId": "2af52e49-c885-44a6-ae28-376df92f3816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = torch.randn(2,2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5029, -0.1096],\n",
            "        [ 0.2689, -0.2071]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSyy1LtGWYJd",
        "colab_type": "code",
        "outputId": "293c70a2-b0b3-4efe-df83-6b30fa625cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y = x**2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2529, 0.0120],\n",
            "        [0.0723, 0.0429]], grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD0INLSUWYJ0",
        "colab_type": "text"
      },
      "source": [
        "Ниже можно увидеть, в результате какой операции была создана `y`, операция возведения в степень `PowBackward0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SRJR7f2WYKA",
        "colab_type": "code",
        "outputId": "9b6ad0d4-8319-4ae4-a851-0c44c6d8fd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## grad_fn shows the function that generated this variable\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PowBackward0 object at 0x7efcefbe5400>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQy92PCHWYKT",
        "colab_type": "text"
      },
      "source": [
        "Модуль autograd отслеживает эти операции и знает, как рассчитать градиент для каждой из них. Таким образом, он может рассчитывать градиенты для цепочки операций относительно любого тензора. Давайте уменьшим тензор `y` до скалярного значения, среднего."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofXDauCTWYKY",
        "colab_type": "code",
        "outputId": "46f3c919-af46-46ad-deb2-0cf471b5dc4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "z = y.mean()\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-h99lLEWYKs",
        "colab_type": "text"
      },
      "source": [
        "Вы можете проверить градиенты для `x` и` y`, но сейчас они пусты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4YeNGdxWYK3",
        "colab_type": "code",
        "outputId": "f8407960-de8a-4ed5-ad51-70e672dc4c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u1gAmAVWYLX",
        "colab_type": "text"
      },
      "source": [
        "Чтобы вычислить градиенты, вам нужно запустить метод `.backward` для переменной, например,` z`. Это вычислит градиент `z` по ` x`\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDgUuleoWYLa",
        "colab_type": "code",
        "outputId": "01cf2b0e-9265-4642-f5d4-6fb1f00976f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "z.backward()\n",
        "print(x.grad)\n",
        "print(x/2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2514, -0.0548],\n",
            "        [ 0.1344, -0.1035]])\n",
            "tensor([[-0.2514, -0.0548],\n",
            "        [ 0.1344, -0.1035]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTdniVgcWYLj",
        "colab_type": "text"
      },
      "source": [
        "Эти вычисления градиентов особенно полезны для нейронных сетей. Для обучения нам нужны градиенты ошибки по весам. С помощью PyTorch мы пропускаем данные вперед по сети, чтобы рассчитать ошибку, а затем возвращаемся назад, чтобы рассчитать градиенты относительно ошибки. Когда у нас есть градиенты, мы можем сделать шаг градиентного спуска."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjzN0malWYLm",
        "colab_type": "text"
      },
      "source": [
        "## Loss и Autograd вместе\n",
        "\n",
        "Когда мы создаем сеть с помощью PyTorch, все параметры модели инициализируются с `require_grad = True`. Это означает, что когда мы вычисляем ошибку `loss` и вызываем функцию `loss.backward()`, вычисляются градиенты для параметров модели. Эти градиенты используются для обновления весов с градиентным спуском. Ниже вы можете увидеть пример расчета градиентов с помощью обратного прохода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT37n4F7WYLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "logits = model(images)\n",
        "loss = criterion(logits, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64L_uLkQWYLy",
        "colab_type": "code",
        "outputId": "73dd0b0a-ad5b-46eb-9807-82c3d1706600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print('Before backward pass: \\n', model[0].weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('After backward pass: \\n', model[0].weight.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before backward pass: \n",
            " None\n",
            "After backward pass: \n",
            " tensor([[-0.0062, -0.0062, -0.0062,  ..., -0.0062, -0.0062, -0.0062],\n",
            "        [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
            "        [-0.0021, -0.0021, -0.0021,  ..., -0.0021, -0.0021, -0.0021],\n",
            "        [ 0.0028,  0.0028,  0.0028,  ...,  0.0028,  0.0028,  0.0028]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGYJabtOWYL7",
        "colab_type": "text"
      },
      "source": [
        "## Обучаем сеть!\n",
        "\n",
        "Осталась последняя часть, которая нам нужна чтобы начать тренировать модель - оптимизатор, который мы будем использовать для обновления весов с помощью градиентов. Мы получаем их из [`optim` пакета] PyTorch (https://pytorch.org/docs/stable/optim.html). Например, мы можем использовать стохастический градиентный спуск с `optim.SGD`. Вы можете увидеть, как определить оптимизатор ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL5P4fJWWYL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErJGuo_XWYMH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Теперь мы знаем, как использовать отдельные части, поэтому пришло время посмотреть, как они работают вместе. Давайте рассмотрим только один шаг обучения, прежде чем перебирать все данные. Общий процесс с PyTorch:\n",
        "\n",
        "* Сделать прямой проход через сеть\n",
        "* Использовать выход сети для расчета ошибки\n",
        "* Выполнить обратный проход по сети с помощью loss.backward () для вычисления градиентов.\n",
        "* Сделать шаг с оптимизатором, чтобы обновить веса\n",
        "\n",
        "Ниже я пройду один тренировочный шаг и распечатаю веса и градиенты, чтобы вы могли увидеть, как они меняются. Обратите внимание, что у меня есть строка кода `optimizer.zero_grad ()`. Когда вы делаете несколько обратных проходов с одинаковыми параметрами, градиенты накапливаются. Это означает, что вам нужно обнулять градиенты на каждом тренировочном проходе, иначе вы сохраните градиенты от предыдущих тренировочных партий."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUtDPCz3WYMJ",
        "colab_type": "code",
        "outputId": "ac45a178-2f6c-4608-bd9f-90c398ddeb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print('Initial weights - ', model[0].weight)\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(64, 784)\n",
        "\n",
        "# Clear the gradients, do this because gradients are accumulated\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass, then backward pass, then update weights\n",
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Gradient -', model[0].weight.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights -  Parameter containing:\n",
            "tensor([[-0.0144,  0.0010,  0.0070,  ..., -0.0006, -0.0069, -0.0077],\n",
            "        [-0.0092, -0.0259,  0.0253,  ...,  0.0125,  0.0074,  0.0353],\n",
            "        [ 0.0106, -0.0168,  0.0047,  ..., -0.0157,  0.0157, -0.0318],\n",
            "        ...,\n",
            "        [ 0.0338,  0.0296,  0.0142,  ..., -0.0114, -0.0218,  0.0091],\n",
            "        [ 0.0044, -0.0168, -0.0176,  ...,  0.0332,  0.0268, -0.0192],\n",
            "        [ 0.0183,  0.0037,  0.0250,  ...,  0.0156, -0.0012, -0.0301]],\n",
            "       requires_grad=True)\n",
            "Gradient - tensor([[-1.5018e-03, -1.5018e-03, -1.5018e-03,  ..., -1.5018e-03,\n",
            "         -1.5018e-03, -1.5018e-03],\n",
            "        [-2.6457e-03, -2.6457e-03, -2.6457e-03,  ..., -2.6457e-03,\n",
            "         -2.6457e-03, -2.6457e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 1.0704e-04,  1.0704e-04,  1.0704e-04,  ...,  1.0704e-04,\n",
            "          1.0704e-04,  1.0704e-04],\n",
            "        [-2.2410e-03, -2.2410e-03, -2.2410e-03,  ..., -2.2410e-03,\n",
            "         -2.2410e-03, -2.2410e-03],\n",
            "        [ 3.9565e-05,  3.9565e-05,  3.9565e-05,  ...,  3.9565e-05,\n",
            "          3.9565e-05,  3.9565e-05]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar4p6CduWYMT",
        "colab_type": "code",
        "outputId": "fad30915-84c2-4a16-aa00-e808be9b1652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Take an update step and few the new weights\n",
        "optimizer.step()\n",
        "print('Updated weights - ', model[0].weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated weights -  Parameter containing:\n",
            "tensor([[-0.0144,  0.0010,  0.0070,  ..., -0.0006, -0.0069, -0.0077],\n",
            "        [-0.0092, -0.0259,  0.0253,  ...,  0.0125,  0.0074,  0.0354],\n",
            "        [ 0.0106, -0.0168,  0.0047,  ..., -0.0157,  0.0157, -0.0318],\n",
            "        ...,\n",
            "        [ 0.0338,  0.0296,  0.0142,  ..., -0.0114, -0.0219,  0.0091],\n",
            "        [ 0.0044, -0.0168, -0.0176,  ...,  0.0332,  0.0268, -0.0192],\n",
            "        [ 0.0183,  0.0037,  0.0250,  ...,  0.0156, -0.0012, -0.0301]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKEj6cvvWYMe",
        "colab_type": "text"
      },
      "source": [
        "### Обучение\n",
        "\n",
        "Теперь мы поместим этот алгоритм в цикл, чтобы модель могла просмотреть все изображения. Некоторая терминология, один проход по всему набору данных называется *эпохой*. Итак, здесь мы собираемся пройтись по `trainloader`, чтобы получить наши тренировочные батчи. Для каждого батча, мы выполняем проход, где рассчитываем потери, делаем обратный проход и обновляем весовые коэффициенты.\n",
        "\n",
        "> **Упражнение:** Реализовать обучающий проход (туда-назад) для нашей сети. Если вы реализовали это правильно, вы должны увидеть уменьшение ошибки обучения с каждой эпохой."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnkyz2tZWYMi",
        "colab_type": "code",
        "outputId": "0c18ef7b-bb1c-405e-dbb5-361139c55823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "## Your solution here\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
        "#output = model(images)\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    \n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        \n",
        "        # TODO: Training pass\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "      print(f\"Training loss:  {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss:  0.6140206825297906\n",
            "Training loss:  0.2790490790669407\n",
            "Training loss:  0.21097329275996318\n",
            "Training loss:  0.1675457457886703\n",
            "Training loss:  0.1411747891487661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrovpYgUWYMs",
        "colab_type": "text"
      },
      "source": [
        "Когда модель обучена, мы можем посмотреть на предсказания:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy9ZN-IQWYMv",
        "colab_type": "code",
        "outputId": "1911e7e4-89d4-442c-afda-a5cd749775fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import helper\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "img = images[1].view(1, 784)\n",
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "ps\n",
        "#elper.view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0481, 0.8673, 0.9585, 1.1332, 1.1151, 0.9158, 0.9815, 1.1369, 0.8517,\n",
              "         0.9316]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5LEAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so7xmju_WYNI",
        "colab_type": "text"
      },
      "source": [
        "Шикарно"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkT8fqHmWYNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}